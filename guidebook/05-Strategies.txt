

Strategies for putting linked data into practice for the archivist

  "If you to go to Rome for a day, then walk to the Colosseum and
  Vatican City. Everything you see along the way will be extra. If
  you to go to Rome for a few days, do everything you would do in a
  single day, eat and drink in a few cafes, see a few fountains,
  and go to a museum of your choice. For a week, do everything you
  would do in a few days, and make one or two day-trips outside
  Rome in order to get a flavor of the wider community. If you can
  afford two weeks, then do everything you would do in a week, and
  in addition befriend somebody in the hopes of establishing a
  life-long relationship." 

When you read a guidebook about Rome -- or any travel guidebook -- there are simply too many listed things to see & do. Nobody can see all the sites, visit all the museums, walk all the tours, nor eat at all the restaurants. It is literally impossible to experience everything a place like Rome has to offer. So it is with linked data. Despite this fact, if you were to do everything linked data had to offer, then you would do all of things on the following list starting at the first item, going all the way down to evaluation, and repeating the process over and over:

  1. design the structure your URIs
  2. select/design your ontology & vocabularies -- model your data
  3. map and/or migrate your existing data to RDF
  4. publish your RDF as linked data
  5. create a linked data application
  6. harvest other people's data and create another application
  7. evaluate
  8. repeat

Given that it is quite possible you do not plan to immediately dive head-first into linked data, you might begin by getting your feet wet or dabbling in a bit of experimentation. That being the case, here are a number of different "itineraries" for linked data implementation. Think of them as strategies. They are ordered from least costly and most modest to greatest expense and completest execution: 
  
  1. Rome in a day - Maybe you can't afford to do anything right now, but if you have gotten this far in the guidebook, then you know something about linked data. Discuss (evaluate) linked data with with your colleagues, and consider revisiting the topic a year.
  
  2. Rome in three days - If you want something relatively quick and easy, but with the understanding that your implementation will not be complete, begin migrating your existing data to RDF. Use XSLT to transform your MARC or EAD files into RDF serializations, and publish them on the Web. Use something like OAI2RDF to make your OAI repositories (if you have them) available as linked data. Use something like D2RQ to make your archival description stored in databases accessible as linked data. Create a triple store and implement a SPARQL endpoint. As before, discuss linked data with your colleagues. 
  
  3. Rome in week - Being publishing RDF, but at the same time think hard about and document the structure of your future RDF's URIs as well as the ontologies & vocabularies you are going to use. Discuss it with your colleagues. Migrate and re-publish your existing data as RDF using the documentation as a guide. Re-implement your SPARQL endpoint. Discuss linked data not only with your colleagues but with people outside archival practice. 
  
  4. Rome in two weeks - First, do everything you would do in one week. Second, supplement your triple store with the RDF of others'. Third, write an application against the triple store that goes beyond search. In short, tell stories and you will be discussing linked data with the world, literally.


Rome in a day

  "If you to go to Rome for a day, then walk to the Colosseum and
  Vatican City. Everything you see along the way will be extra."

Linked data is not a fad. It is not a trend. It makes a lot of computing sense, and it is a modern way of fulfilling some the goals of archival practice. Just like Rome, it is not going away. An understanding of what linked data has to offer is akin to experiencing Rome first hand. Both will ultimately broaden your perspective. Consequently it is a good idea to make a concerted effort to learn about linked data, as well as visit Rome at least once. Once you have returned from your trip, discuss what you learned with your friends, neighbors, and colleagues. The result will be enlightening everybody.

The previous sections of this book described what linked data is and why it is important. The balance of book describes more of the how's of linked data. For example, there is a glossary to help reenforce your knowledge of the jargon. You can learn about HTTP "content negotiation" to understand how actionable URIs can return HTML or RDF depending on the way you instruct remote HTTP servers. RDF stands for "Resource Description Framework", and the "resources" are represented by URIs. A later section of the book describes ways to design the URIs of your resources. Learn how you can transform existing metadata records like MARC or EAD into RDF/XML, and then learn how to put the RDF/XML on the Web. Learn how to exploit your existing databases (such as the one's under Archon, Archivist's Toolkit, or ArchiveSpace) to generate RDF. If you are the Do It Yourself type, then play with and explore the guidebook's tool section. Get the gentlest of introductions to searching RDF using a query language called SPARQL. Learn how to read and evaluate ontologies & vocabularies. They are manifested as XML files, and they are easily readable and visualizable using a number of programs. Read about and explore applications using RDF as the underlying data model. There are a growing number of them. The book includes a complete publishing system written in Perl, and if you approach the code of the publishing system as if it were a theatrical play, then the "scripts" read liked scenes. (Think of the scripts as if they were a type of poetry, and they will come to life. Most of the "scenes" are less than a page long. The poetry even includes a number of refrains. Think of the publishing system as if it were a one act play.) If you want to read more, and you desire a vetted list of books and articles, then a later section lists a set of further reading.

After you have spent some time learning a bit more about linked data, discuss what you have learned with your colleagues. There are many different aspects of linked data publishing, such as but not limited to:

  * allocating time and money 
  * analyzing the RDF of yours as well as others
  * articulating policies
  * cleaning and improving RDF 
  * collecting and harvesting the RDF of others
  * deciding what ontologies & vocabularies to use
  * designing local URIs
  * enhancing RDF triples stores by asserting additional relationships
  * finding and identifying URIs for the purposes of linking
  * making RDF available on the Web (SPARQL, RDFa, data dumps, etc.)
  * project management
  * provisioning value-added services against RDF (catalogs, finding aids, etc.)
  * storing RDF in triple stores

In archival practice, each of these things would be done by different sets of people: archivists & content specialists, administrators & managers, computer programers & systems administrators, metadata experts & catalogers. Each of these sets of people have a piece of the publishing puzzle and something significant to contribute to the work. Each of these sets of people plays a key and indispensable role in linked data publishing:

  * archivists & content specialists - These are the people who understand the “aboutness” of a particular collection. These are the people who understand and can thoroughly articulate the significance of a collection. They know how and why particular things belong in a collection. They are able to answer questions about the collection as all as tell stories against it. 

  * administrators & managers - 
  
  * metadata specialists & catalogers - These are people who understand data about data. Not only do they understand the principles of controlled vocabularies and authority lists, but they are also familiar with a wide variety of such lists, specifically as they are represented on the Web. In linked data there are fewer descriptive cataloging “rules”. Nevertheless, the way the ontologies of linked data can be used need to be interpreted, and this interpretation needs to be consistent. Metadata specialists understand these principles.

  * computer programers & systems administrators - Not only are these the people who have a fundamental understanding of what computer can and cannot do, but they also know how to put this understanding into practice. At the very least, the computer technologists need to understand a myriad of data structures and how to convert them into different data structures. Converting MARC 21 into MARCXML. Transforming EAD into HTML. Reporting against a relational database to create serialized RDF. These tasks required computer programming skills, but not necessarily any one in particular. Any modern programming language (Java, PHP, Python, Ruby, etc.) includes the necessary function to complete the tasks. 

Read about linked data. Learn about linked data. Bring these sets of people together discuss what you have learned. With this in mind, articulate some goals — broad targets of things you would like to accomplish. Some of them might include:

  * making your archival collections more widely accessible
  * working with others to build virtual collections of like topics or formats
  * incorporating your archival descriptions into public spaces like Wikipedia
  * integrating your collections into local teaching, learning, and research activities
  * increasing the awareness of your archive to benefactors
  * increasing the computer technology skills of fellow archivists

The what of your objectives are not so much identified with nouns as they are action verbs, such as: write, evaluate, implement, examine, purchase, hire, prioritize, list, delete, acquire, discuss, share, find, compare & contrast, stop, start, complete, continue, describe, edit, updated, create, purchase, upgrade, etc. The what of your objective is in the doing.

After discussing these sorts of issues, at the very least you will have a better collective understanding of the possibilities. If you don't plan to "go to Rome" right away, you might decide to reconsider the "vacation" at another time.

  "Even Michelangelo, when he painted the Sisten Chapel, worked
  with a team of people each possessing a complementary set of
  skills. Each had something different to offer, and the
  discussion between themselves was key to their success."


Rome in three days

  "If you to go to Rome for a few days, do everything you would do
  in a single day, eat and drink in a few cafes, see a few
  fountains, and go to a museum of your choice."

Linked data in archival practice is not new. Others have been here previously. You can benefit from their experience and begin publishing linked data right now using tools with which you are probably already familiar. For example, you probably have EAD files, sets of MARC records, or metadata saved in database applications. Using existing tools, you can transform this content into RDF and put the result on the Web, thus publishing your information as linked data.


EAD

If you have used EAD to describe your collections, then you can easily make your descriptions available as valid linked data, but the result will be less than optimal. This is true not for a lack of technology but rather from the inherent purpose and structure of EAD files.

A few years ago an organisation in the United Kingdom called the Archive's Hub was funded by a granting agency called JISC to explore the publishing of archival descriptions as linked data. One of the outcomes of this effort was the creation of an XSL stylesheet transforming EAD into RDF/XML. The terms used in the stylesheet originate from quite a number of standardized, widely accepted ontologies, and with only the tiniest bit configuration / customization the stylesheet can transform a generic EAD file into valid RDF/XML. The resulting XML files can then be made available on a Web server or incorporated into a triple store. This goes a long way to publishing archival descriptions as linked data. The only additional things needed are a transformation of EAD into HTML and the configuration of a Web server to do content negotiation between the XML and HTML. 

For the smaller archive with only a few hundred EAD files whose content does not change very quickly, this is a simple, feasible, and practical solution to publishing archival descriptions as linked data. With the exception of doing some content negotiation, this solution does not require any computer technology that is not already being used in archives, and it only requires a few small tweaks to a given workflow:

  1. implement a content negotiation solution 
  2. create and maintain EAD file 
  3. transform EAD into RDF/XML 
  4. transform EAD into HTML 
  5. save the resulting XML and HTML files on a Web server 
  6. go to step #2

EAD is a combination of narrative description and a hierarchal inventory list, and this data structure does not lend itself very well to the triples of linked data. For example, EAD headers are full of controlled vocabularies terms but there is no way to link these terms with specific inventory items. This is because the vocabulary terms are expected to describe the collection as a whole, not individual things. This problem could be overcome if each individual component of the EAD were associated with controlled vocabulary terms, but this would significantly increase the amount of work needed to create the EAD files in the first place.

The common practice of using literals ("strings") to denote the names of people, places, and things in EAD files would also need to be changed in order to fully realize the vision of linked data. Specifically, it would be necessary for archivists to supplement their EAD files with commonly used URIs denoting subject headings and named authorities. These URIs could be inserted into id attributes throughout an EAD file, and the resulting RDF would be more linkable, but the labor to do so would increase, especially since many of the named items will not exist in standardized authority lists.

Despite these short comings, transforming EAD files into some sort of serialized RDF goes a long way towards publishing archival descriptions as linked data. This particular process is a good beginning and outputs valid information, just information that is not as linkable as possible. This process lends itself to iterative improvements, and outputting something is better than outputting nothing. But this particular proces is not for everybody. The archive whose content changes quickly, the archive with copious numbers of collections, or the archive wishing to publish the most compliant linked data possible will probably not want to use EAD files as the root of their publishing system. Instead some sort of database application is probably the best solution.


MARC

In some ways MARC lends it self very well to being published via linked data, but in the long run it is not really a feasible data structure.

Converting MARC into serialized RDF through XSLT is at least a two step process. The first step is to convert MARC into MARCXML. This can be done with any number of scripting languages and toolboxes. The second step is to use a stylesheet such as the one created by Stefano Mazzocchi to transform the MARCXML into RDF/XML. [1] From there a person could save the resulting XML files on a Web server, enhance access via content negotiation, and called it linked data.

Unfortunately, this particular approach has a number of drawbacks. First and foremost, the MARC format had no place to denote URIs; MARC records are made up almost entirely of literals. Sure, URIs can be constructed from various control numbers, but things like authors, titles, subject headings, and added entries will most certainly be strings ("Mark Twain", "Adventures of Huckleberry Finn", "Bildungsroman", or "Samuel Clemans"), not URIs. This issue can be overcome if the MARCXML were first converted into MODS and URIs were inserted into id or xlink attributes of bibliographic elements, but this is extra work. If an archive were to take this approach, then it would also behoove them to use MODS as their data structure of choice, not MARC. Continually converting from MARC to MARCXML to MODS would be expensive in terms of time. Moreover, with each new conversion the URIs from previous iterations would need to be re-created.

[1] stylesheet by Mazzocchi - https://github.com/dltj/MARC-MODS-RDFizer/blob/master/stylesheets/mods2rdf.xslt


EAC-CPF

Encoded Archival Context for Corporate Bodies, Persons, and Families (EAC-CPF) goes a long way to implementing a named authority database that could be linked from archival descriptions. These XML files could easily be transformed into serialized RDF and therefore linked data. The resulting URIs could then be incorporated into archival descriptions making the descriptions richer and more complete.

For example the FindAndConnect site in Australia uses EAC-CPF under the hood to disseminate information about people in its collection. [1] Similarly, “SNAC aims to not only make the [EAC-CPF] records more easily discovered and accessed but also, and at the same time, build an unprecedented resource that provides access to the socio-historical contexts (which includes people, families, and corporate bodies) in which the records were created” -- u  More than a thousand EAC-CPF records are available from the RAMP project -- http://demo.rampeditor.info/export.php

  * Library, The standard EAC-CPF is maintained by the Society of American Archivists in partnership with the Berlin State. “Society of American Archivists and the Berlin State Library (http://eac.staatsbibliothek-berlin.de/) - 


[1] FindAndConnect - http://www.findandconnect.gov.au
[2] SNAC - http://socialarchive.iath.virginia.edu


METS, MODS, and perhaps more

If you have archival descriptions in either of the METS or MODS formats, then transforming them into RDF is as far away as your XSLT processor and a content negotiation implementation. As of this writing there do not seem to be any METS to RDF stylesheets, but there are a couple stylesheets for MODS. The biggest issue with these sorts of implementations are the URIs. It will be necessary for archivists to include URIs into as many MODS id or xlink attributes as possible. The same thing holds true for METS files except the id attribute is not designed to hold pointers to external sites.


Databases

Publishing linked data through XML transformation is functional but not optimal. Publishing linked data from a database comes closer to the ideal but requires a greater amount of technical computer infrastructure and expertise. 

Databases -- specifically, relational databases -- are the current best practice for organizing data. As you may or may not know, relational databases are made up of many tables of data joined together with keys. For example, a book may be assigned a unique identifier. The book has many characteristics such as a title, number of pages, size, descriptive note, etc. Some of the characteristics are shared by other books, like authors and subjects. In a relational database these shared characteristics would be saved in additional tables, and they would be joined to a specific book through the use of unique identifiers (keys). Given this sort of data structure, reports can be created from the database describing its content. Similarly, queries can be applied against the database to uncover relationships that may not be apparent at first glance or buried in reports. The power of relational databases lies in the use of keys to make relationships between rows in one table and rows in other tables.

Not coincidently, this is very much the way linked data is expected to be implemented. In the linked data world, the subjects of triples are URIs (think database keys). Each URI is associated with one or more predicates (think the characteristics in the book example). Each triple then has an object, and these objects take the form of literals or other URIs. In the book example, the object could be “Adventures Of Huckleberry Finn” or a URI pointing to Mark Twain. The reports of relational databases are analogous to RDF serializations, and SQL (the relational database query language) is analogous to SPARQL, the query language of RDF triple stores. Because of the close similarity between well-designed relational databases and linked data principles, the publishing of linked data directly from relational databases makes whole lot of sense, but the process requires the combined time and skills of a number of different people: content specialists, database designers, and computer programmers. Consequently, the process of publishing linked data from relational databases may be optimal, but it is more expensive.

Thankfully, most archivists probably use some sort of database to manage their collections and create their finding aids. Moreover, archivists probably use one of three or four tools for this purpose: Archivist’s Toolkit, Archon, ArchivesSpace, or PastPerfect. Each of these systems have a relational database at their heart. Reports could be written against the underlying databases to generate serialized RDF and thus begin the process of publishing linked data. Doing this from scratch would be difficult, as well as inefficient because many people would be starting out with the same database structure but creating a multitude of varying outputs. Consequently, there are two alternatives. The first is to use a generic database application to RDF publishing platform called D2RQ. The second is for the community to join together and create a holistic RDF publishing system based on the database(s) used in archives.

D2RQ is a very powerful software system. [1] It is supported, well-documented, executable on just about any computing platform, open source, focused, functional, and at the same time does not try to be all things to all people. Using D2RQ it is more than possible to quickly and easily publish a well-designed relational database as RDF. The process is relatively simple:

  * download the software 
  * use a command-line utility to map the database
    structure to a configuration file 
  * season the configuration file to taste 
  * run the D2RQ server using the configuration file
    as input thus allowing people or RDF user-agents
    to search and browse the database using linked
    data principles 
  * alternatively, dump the contents of the database
    to an RDF serialization and upload the result
    into your favorite RDF triple store

The downside of D2RQ is its generic nature. It will create an RDF ontology whose terms correspond to the names of database fields. These field names do not map to widely accepted ontologies and therefore will not interact well with communities outside the ones using a specific database structure. Still, the use of D2RQ is quick, easy, and accurate.

The second alternative requires community effort and coordination. The databases of Archivist’s Toolkit, Archon, ArchivesSpace, or Past Perfect could be assumed. The community could then get together and decide on an RDF ontology to use for archival descriptions. The database structure(s) could then be mapped to this ontology. Next, programs could be written against the database(s) to create serialized RDF thus beginning the process of publishing linked data. Once that was complete, the archival community would need to come together again to ensure it uses as many shared URIs as possible thus creating the most functional sets of linked data. This second alternative requires a significant amount of community involvement and wide-spread education. It represents a never-ending process.

[1] D2RQ - http://d2rq.org


Rome in week

  "For a week, do everything you would do in a few days, and make
  one or two day-trips outside Rome in order to get a flavor of the
  wider community."

  
Rome in two weeks

  "If you can afford two weeks, then do everything you would do in
  a week, and in addition befriend somebody in the hopes of
  establishing a life-long relationship."


=====


Strategies

Linked data represents a modern way of making your archival descriptions accessible to the wider world. In that light, it represents a different way of doing things but not necessary a different what of doing things. You will still be doing inventory. You will still be curating collections. You will still be prioritizing what goes and what stays.

5.a Linked data and archival practice: Or, There is more than one way to get there

Two recent experiences have taught me that — when creating some sort of information service — linked data will reside and be mixed in with data collected from any number of Internet techniques. Linked data interfaces will coexist with REST-ful interfaces, or even things as rudimentary as FTP. To the archivist, this means linked data is not the be-all and end-all of information publishing. There is no such thing. To the application programmer, this means you will need to have experience with a ever-growing number of Internet protocols. To both it means, “There is more than one way to get there.”

In October of 2013 I had the opportunity to attend the Semantic Web In Libraries conference.   It was a three-day event attended by approximately three hundred people who could roughly be divided into two equally sized groups: computer scientists and cultural heritage institution employees. The bulk of the presentations fell into two categories: 1) publishing linked data, and 2) creating information services. The publishers talked about ontologies, human-computer interfaces for data creation/maintenance, and systems exposing RDF to the wider world. The people creating information services were invariably collecting, homogenizing, and adding value to data gathered from a diverse set of information services. These information services were not limited to sets of linked data. They also included services accessible via REST-ful computing techniques, OAI-PMH interfaces, and there were probably a few locally developed file transfers or relational database dumps described as well. These people where creating lists of information services, regularly harvesting content from the services, writing cross-walks, locally storing the content, indexing it, providing services against the result, and sometimes republishing any number of “stories” based on the data. For the second group of people, linked data was certainly not the only game in town.

In February of 2014 I had the opportunity to attend a hackathon called GLAM Hack Philly.  A wide variety of data sets were presented for “hacking” against. Some where TEI files describing Icelandic manuscripts. Some was linked data published from the British museum. Some was XML describing digitized journals created by a vendor-based application. Some of it resided in proprietary database applications describing the location of houses in Philadelphia. Some of it had little or no computer-readable structure at all and described plants. Some of it was the wiki mark-up for local municipalities. After the attendees (there were about two dozen of us) learned about each of the data sets we self-selected and hacked away at projects of our own design. The results fell into roughly three categories: geo-referencing objects, creating searchable/browsable interfaces, and data enhancement. With the exception of the resulting hack repurposing journal content to create new art, the results were pretty typical for cultural heritage institutions. But what fascinated me was way us hackers selected our data sets. Namely, the more complete and well-structured the data was the more hackers gravitated towards it. Of all the data sets, the TEI files were the most complete, accurate, and computer-readable. Three or four projects were done against the TEI. (Heck, I even hacked on the TEI files.) The linked data from the British Museum — very well structured but not quite as through at the TEI — attracted a large number of hackers who worked together for a common goal. All the other data sets had only one or two people working on them. What is the moral to the story? There are two of them. First, archivists, if you want people to process your data and do “kewl” things against it, then make sure the data is thorough, complete, and computer-readable. Second, computer programmers, you will need to know a variety of data formats. Linked data is not the only game in town.

In summary, the technologies described in this Guidebook are not the only way to accomplish the goals of archivists wishing to make their content more accessible. Instead, linked data is just one of many protocols in the toolbox. It is open, standards-based, and simpler rather than more complex. On the other hand, other protocols exist which have a different set of strengths and weaknesses. Computer technologists will need to have a larger rather than smaller knowledge of various Internet tools. For archivists, the core of the problem is still the collection and description of content. This — a what of archival practice — continues to remain constant. It is the how of archival practice — the technology — that changes at a much faster pace.

How might you go about accomplishing these goals? What are your objectives? (What method of transportation are you going to use to get where you are going?) How am I going to measure success? In other words, you will need to create an plan, and each item in the plan answers a simple question — Who is going to do what by when? In other word, what people will be responsible for accomplishing the particular objective. Exactly what will they be doing, and by what time will they have it accomplished. Each of these components are described in greater detail below

On the other hand, linked data changes the way your descriptions get expressed and distributed. It is a lot like taking a trip across country. The goal was always to get to the coast to see the ocean, but instead of walking, going by stage coach, taking a train, or driving a car, you will be flying. Along the way you may visit a few cities and have a few layovers. Bad weather may even get in the way, but sooner or later you will get to your destination. Take a deep breath. Understand that the process will be one of learning, and that learning will be applicable in other aspects of your work. The result will be two-fold. First, a greater number of people will have access to your collections, and consequently, more people will will be using your collections. 


Structure your URIs


Select/design your ontology & vocabularies -- model your data

  3. Articulate and implement best practices for publishing RDF - Work with your friends to articulate and document an "application profile". As guidelines and best practices get articulated, implement them by going back to Step #1. In the meantime, continue on to Step #4.



Map and/or migrate your existing data to RDF
  1. Create and maintain RDF - Generate, save, and update sets of RDF statements describing your content. 

Three Cs: Cleanup, Conversion, Consistency

The article entitled Recipes for Enhancing Digital Collections with Linked Data
by Thomas Johnson and Karen Estlund (http://journal.code4lib.org/articles/9214) outlines a number of ways of cleaning up data in content management systems by way of RDF statements. 

clean up steps include:

	1.	Remove noise
	2.	Normalize presentation
	3.	Assign URIs for curation objects
	4.	Map legacy elements to Linked Data vocabularies

As stated by Hillman, the process of moving to linked data is The key to this aug- mentation process involves changing the basic metadata unit from “record” to “statement.” — http://dcpapers.dublincore.org/pubs/article/view/770/766

Problems with data, again from hillman an:

  1. missing data – metadata elements not present in supplied metadata
  2. incorrect data – metadata values not con- forming to standard element use 
  3. confusing data – multiple values crammed into a single metadata element, embedded html tags, etc. 
  4. insufficient data – e.g., no indication of controlled vocabularies used 

Safe transformations include:

  1. remove “noise” – a partial solution to the “incorrect data” problem. For example, we remove metadata with no information value, such as empty metadata elements, metadata elements with values such as “unknown” or “n/a” or consisting entirely of dashes or other punctuation.

  2. detect and identify controlled vocabular- ies in use whenever possible – a partial solution to the “insufficient data” prob- lem. For example, the DCMIType encod- ing scheme is applied to DC “Type” elements when their value is one of the allowed DCMITypes [10]. This works well for small controlled vocabularies; however, it does not scale well to large vocabularies such as LCSH.

  3. normalize metadata presentation – clean up the values: remove double XML en- codings (“&amp;lt;” becomes “&lt;”), extra whitespace (a tab followed by five spaces becomes a single space), etc.


Creating and maintaining metadata is a never-ending process. The items being described can always use elaboration. Collections may increase is size. Rights applied against content may change. Things become digitized, or digitized things are migrated from one format to another. Because of these sorts of things and many others, cleanup, conversion, and consistency are something every metadata specialist needs to keep in mind. 

Cleanup, conversion, and consistency means many things. Does all of your metadata use the same set of one or more vocabularies? Are things spelled correctly? Maybe you used abbreviations in one document but spelled things out in another? Have you migrated your JPEG images to JPEG2000 or TIFF formats? Maybe the EAD DTD has been updated, and you want (need) to migrate your finding aids from one XML format to another? Do all of your finding aids exhibit the same level of detail; are some “thinner” than others? Have you used one form of a person’s name in one document but used another form in a different document? The answers to these sorts of questions point to the need for cleanup, conversion, and consistency. 



5.b. Is your archival description LOD-ready?

Is your archival description LOD-ready? Now? The simple, straight-forward answer is, "Yes." The longer and more complicated answer is, "No. Your data is never 100% linked data ready because the process of archival description is never finished." That said, the balance of the Guide describes what you can do going forward. 

5.c. Identify building blocks: metadata components in archival description that are (or nearly are) ready for linking.

5.d. Readiness: Making small changes in practice to make your description LOD-ready.


Publish your RDF
  2. Publish RDF - No matter what kind of RDF you are able to create, make it available on the Web.
1. Do simple publishing - At its very root, linked data is about making your data available for others to harvest and use. While the “killer linked data application” has seemingly not reared its head, this does not mean you ought not make your data available at linked data. You won’t see the benefits immediately, but sooner or later (less than 5 years from now), you will see your content creeping into the search results of Internet indexes, into the work of both computational humanists and scientists, and into the hands of esoteric hackers creating one-off applications. Internet search engines will create “knowledge graphs”, and they will include links to your content. The humanists and scientists will operate on your data similarly. Both will create visualizations illustrating trends. They will both quantifiably analyze your content looking for patterns and anomalies. Both will probably create network diagrams demonstrating the flow and interconnection of knowledge and ideas through time and space. The humanist might do all this in order to bring history to life or demonstrate how one writer influenced another. The scientist might study ways to efficiently store your data, easily move it around the Internet, or connect it with data set created by their apparatus. The hacker (those are the good guys) will create flashy-looking applications that many will think are weird and useless, but the applications will demonstrate how the technology can be exploited. These applications will inspire others, be here one day and gone the next, and over time, become more useful and sophisticated. 

5.e. What you can do now if you have (done)

Each of the sections below outline how you can participate in linked data if currently have any number of metadata file formats (MARC, EAD, etc.).


Create an RDF application
  5. Develop services against harvested RDF - Evaluate (curate) the collection. 

 What can you do with linked data once it is created? Here are three use cases:

2. Create a union catalog - If you make your data available as linked data, and if you find at least one other archive who is making their data available as linked data, then you can find a third somebody who will combine them into a triple store and implement a rudimentary SPARQL interface against the union. Once this is done a researcher could conceivably search the interface for a URI to see what is in both collections. The absolute imperative key to success for this to work is the judicious inclusion of URIs in both data sets. This scenario becomes even more enticing with the inclusion of two additional things. First, the more collections in the triple store the better. You can not have enough collections in the store. Second, the scenario will be even more enticing when each archive publishes their data using similar ontologies as everybody else. Success does not hinge on similar ontologies, but success is significantly enhanced. Just like the relational databases of today, nobody will be expected to query them using their native query language (SQL or SPARQL). Instead the interfaces will be much more user-friendly. The properties of classes in ontologies will become facets for searching and browsing. Free text as well as fielded searching via drop-down menus will become available. As time goes on and things mature, the output from these interfaces will be increasingly informative, easy-to-read, and computable. This means the output will answer questions, be visually appealing, as well as be available in one or more formats for other computer programs to operate upon.  

3. Tell a story - You and your hosting institution(s) have something significant to offer. It is not just about you and your archive but also about libraries, museums, the local municipality, etc. As a whole you are a local geographic entity. You represent something significant with a story to tell. Combine your linked data with the linked data of others in your immediate area. The ontologies will be a total hodgepodge, at least at first. Now provide a search engine against the result. Maybe you begin with local libraries or museums. If you work in an academic setting, then maybe you begin with other academic departments across campus. Allow people to search the interface and bring together the content of everybody involved. Do not just provide lists of links in search results, but instead create knowledge graphs. Supplement the output of search results with the linked data from Wikipedia, Flickr, etc. In a federated search sort of way, supplement the output with content from other data feeds such as (licensed) bibliographic indexes or content harvested from OAI-PMH repositories. Identify complementary content from further afield. Figure out a way for you and they to work together to create a newer, more complete set of content. Creating these sorts of things on-the-fly will be challenging. On the other hand, you might implement something that is more iterative and less immediate, but more thorough and curated if you were to select a topic or theme of interest, and do your own searching and story telling. The result would be something that is at once a Web page, a document designed for printing, or something importable into another computer program. 

4. Create new knowledge - Create an inference engine, turn it against your triple store, and look for relationships between distinct sets of URIs that weren't previously apparent. Here's one way how: 

  1. allow the reader to select an actionable URI of personal
     interest, ideally a URI from the set of URIs you curate

  2. submit it an HTTP server or SPARQL endpoint and request RDF as
     output

  3. save the output to a local store

  4. for each subject and object URI found the output, go to
     Step #2

  5. go to step #2 n times for each newly harvested URI in the store
     where n is a reader-defined integer greater than 1; in other
     words, harvest more and more URIs, predicates, and literals
     based on the previously harvested URIs

  6. create a set of human readable services/reports against the
     content of the store, and think of these services/reports akin to
     finding aids, reference materials, or museum exhibits of the
     future: Example services/reports might include:

      * hierarchal lists of all classes and properties - This
        would be a sort of semantic map. Each item on the map
        would be clickable allowing the reader to read more and
        drill down.

      * text mining reports - collect into a single "bag of
        words" all the literals saved in the store and create:
        word clouds, alphabetical lists, concordances,
        bibliographies, directories, gazetteers, tabulations of
        parts of speech, named entities, sentiment analyses,
        topic models, etc.

      * maps - use place names and geographic coordinates to
        implement a geographic information service

      * audio-visual mash-ups - bring together all the media
        information and create things like slideshows, movies,
        analyses of colors, shapes, patterns, etc.

      * search interfaces - implement a search interface
        against the result, SPARQL or otherwise

      * facts - remember SPARQL queries can return more than
        just lists. They can return mathematical results such
        as sums, ratios, standard deviations, etc. It can also
        return Boolean values helpful in answering yes/no
        questions. You could have a set of canned fact queries
        such as, how many ontologies are represented in the
        store. Is the number of ontologies greater than 3? Are
        there more than 100 names represented in this set? The
        count of languages used in the set, etc.

  7. Allow the reader to identify a new URI of personal interest,
     specifically one garnered from the reports generated in Step #5.

  8. Go to Step #2, but this time have the inference engine be more
     selective by having it try to crawl back to your namespace and
     set of locally curated URIs.

  9. Return to the reader the URIs identified in Step #7, and by
     consequence, these URIs ought to share some of the same
     characteristics as the very first URI; you have implemented a
     "find more like this one" tool. You, as curator of the collection
     of URIs might have thought the relations between the first URI
     and set of final URIs was obvious, but those relationships would
     not necessarily be obvious to the reader, and therefore new
     knowledge would have been created or brought to light.

 10. If there are no new URIs from Step #7, then go to Step #6
     using the newly harvested content.

 11. Done - if a system were created such as the one above, then
     the reader would quite likely have acquired some new knowledge,
     and this would be especially true the greater the size of n in
     Step #5. 



Harvest other people's data and create an application

  4. Harvest RDF - Collect RDF from the Web, and some of the collection may be some of your own content.

Evaluate


Repeat
  6. Go to Step #1 - This is a never-ending process. 


=====

Design pattern for archives

  The Use Cases for LiAM include scenarios in which understanding
  of or access to archival collections are made more difficult or
  cumbersome when description is strictly limited to the
  traditional finding aid. In describing these scenarios, LiAM
  staff and Working Group members drew on first-hand experience
  managing and describing collections as well as helping
  researchers use the collections in our care.

  The Use Cases for LiAM include scenarios in which understanding
  of or access to archival collections are made more difficult or
  cumbersome when description is strictly limited to the
  traditional finding aid. In describing these scenarios, LiAM
  staff and Working Group members drew on first-hand experience
  managing and describing collections as well as helping
  researchers use the collections in our care.

  General Problem Statement for LiAM

  From the grant narrative:

	  Most finding aids — archival collection descriptions often
	  encoded in EAD — are hierarchical and linear narrative documents
	  that take a top-down approach to archival description. They start
	  by describing an archival collection as a whole, its creator(s),
	  and how the collection is organized.

	  From there finding aids typically describe series, subseries, and
	  on down to the lowest level of description. The linear flow of
	  the traditional finding aid closely mirrors the physical
	  arrangement of the documents in hand, serving both as a
	  description of the collection and as a map to where records are
	  physically located on the actual shelves or within the actual
	  boxes and folders.

	  The archival principle of provenance provides the theoretical
	  underpinning for this hierarchical formulation of the finding
	  aid. This principle dictates that “records of different origins
	  (provenance) be kept separate to preserve their context.”1
	  Archivists use the finding aid as their main tool to adhere to
	  this principle in their management and description of their
	  archival holdings. The finding aid supports provenance by hewing
	  closely to the notion of original order.

  By crafting finding aids to provide a representation of how the
  records creator arranged and ordered his, her, or its records,
  archives aim to preserve “existing relationships and evidential
  significance that can be inferred from the context of the
  records.” Archivists use finding aids to provide intellectual and
  physical control of their holdings, and to give users the means
  to discover records and objects within collections and understand
  the way items and groupings of documents relate to each other.

  Complexity is not new to archival collections. However, we feel
  that changes in recordkeeping from creation to use is increasing
  the complexity of the descriptive work required of archivists and
  challenging the effectiveness of traditional descriptive modes.
  We feel the need to explore alternative descriptive approaches
  that can meaningfully represent that complexity.

  In most cases, archivists know or become aware of vital
  contexualizing information as part of arrangement and description
  work. However, they often struggle to articulate this information
  it to users in ways that are easily accessible and meaningful and
  that don’t require excessive supplemental narrative description.

  Some of these complexities are:

  * Name - 
  * Question - "An individual record or series of records often have simultaneous significance in multiple contexts. (Anne Sauer)How do you signify simultaneous records when they might have multiple contexts?"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Record creators have multifaceted relationships to different records and series of records.(Anne Sauer) How do you model record creators with different series of records?"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Documentation of a function often spans provenance-based records series. (Anne Sauer)How do you link functions when provenance-based records span series?"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Documentation of an event often spans provenance-based records series. (Anne Sauer)How do you link events when provenance-based records span series?"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Intentional filing systems are often no longer employed as a file management strategy, undermining the affordance of original order as a conceptual basis for description. (Anne Sauer) How do you model the intentional filing system? "
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Because women may change their names more frequently than men in Western culture, this project will be particularly helpful in connect collections that may end up being un-connected due to different names for the same person. An interesting example of scattered collections that would have been brought together are the William Cameron Blackett personal archive at the Harvard University Archives with two collections of papers of his daughter (Priscilla Blackett Dewey Houghton) described as the Priscilla B. Dewey papers at the Harvard Theatre Collection, Houghton Library and the papers of Priscilla Dewey Houghton at the Schlesinger Library. (Juliana Kuipers)"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "How about linking by function? Responsibility and authority for a particular administrative function may move from one office to another. For example, student discipline may move from a President’s office to a Dean of a school, to the Dean of Students, and then to a disciplinary board. (Kate Bowers)"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "Email addresses are another interesting point of convergence/divergence. Multiple aliases for the same email account may exist simultaneously, one person may have many email accounts. (Skip Kendall)"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

  * Name - 
  * Question - "@Skip, excellent point. Social media/Internet identities in general are all over the place. I’m just imagining trying to link the history of, well, me over the years: a number of e-mail addresses, twitter identities, blog identities, openID, etc. Of course doing such a thing opens up some interesting archival ethical questions. In many Internet subcultures there a reasonable concept of an open secret connecting somebody’s legal name with an Internet identity name. Presumably archival ethics would emphasize the “secret” part rather than the “open”whenever possible and avoid linking. Huh, thought-provoking.  @Kate, definitely true! And an important thing to track.  @Julianna, indeed, and there are other name changes to track as well, such as people who take Western names when they change countries, etc. (Deborah Kaplan)"
  * Context - 
  * Solution - 
  * Example - 
  * Discussion - 

-----


