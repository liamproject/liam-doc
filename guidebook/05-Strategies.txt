

Strategies for putting linked data into practice for the archivist

  "If you to go to Rome for a day, then walk to the Colosseum and
  Vatican City. Everything you see along the way will be extra. If
  you to go to Rome for a few days, do everything you would do in a
  single day, eat and drink in a few cafes, see a few fountains,
  and go to a museum of your choice. For a week, do everything you
  would do in a few days, and make one or two day-trips outside
  Rome in order to get a flavor of the wider community. If you can
  afford two weeks, then do everything you would do in a week, and
  in addition befriend somebody in the hopes of establishing a
  life-long relationship." 

When you read a guidebook about Rome -- or any travel guidebook -- there are simply too many listed things to see & do. Nobody can see all the sites, visit all the museums, walk all the tours, nor eat at all the restaurants. It is literally impossible to experience everything a place like Rome has to offer. So it is with linked data. Despite this fact, if you were to do everything linked data had to offer, then you would do all of things on the following list starting at the first item, going all the way down to evaluation, and repeating the process over and over:

  1. design the structure your URIs
  2. select/design your ontology & vocabularies -- model your data
  3. map and/or migrate your existing data to RDF
  4. publish your RDF as linked data
  5. create a linked data application
  6. harvest other people's data and create another application
  7. evaluate
  8. repeat

Given that it is quite possible you do not plan to immediately dive head-first into linked data, you might begin by getting your feet wet or dabbling in a bit of experimentation. That being the case, here are a number of different "itineraries" for linked data implementation. Think of them as strategies. They are ordered from least costly and most modest to greatest expense and completest execution: 
  
  1. Rome in a day - Maybe you can't afford to do anything right now, but if you have gotten this far in the guidebook, then you know something about linked data. Discuss (evaluate) linked data with with your colleagues, and consider revisiting the topic a year.
  
  2. Rome in three days - If you want something relatively quick and easy, but with the understanding that your implementation will not be complete, begin migrating your existing data to RDF. Use XSLT to transform your MARC or EAD files into RDF serializations, and publish them on the Web. Use something like OAI2RDF to make your OAI repositories (if you have them) available as linked data. Use something like D2RQ to make your archival description stored in databases accessible as linked data. Create a triple store and implement a SPARQL endpoint. As before, discuss linked data with your colleagues. 
  
  3. Rome in week - Begin publishing RDF, but at the same time think hard about and document the structure of your future RDF's URIs as well as the ontologies & vocabularies you are going to use. Discuss it with your colleagues. Migrate and re-publish your existing data as RDF using the documentation as a guide. Re-implement your SPARQL endpoint. Discuss linked data not only with your colleagues but with people outside archival practice. 
  
  4. Rome in two weeks - First, do everything you would do in one week. Second, supplement your triple store with the RDF of others'. Third, write applications against the triple store that goes beyond search. These services will be of two types: services for curating the collection information, and services for using the collection. The former services are primarily for archivists and subject speciallists. The later services are primarily intended for everybody from the general public to the academic scholar. These services will be akin to the telling of stories, and you will be discussing linked data with the world, literally.


Rome in a day

  "If you to go to Rome for a day, then walk to the Colosseum and
  Vatican City. Everything you see along the way will be extra."

Linked data is not a fad. It is not a trend. It makes a lot of computing sense, and it is a modern way of fulfilling some the goals of archival practice. Just like Rome, it is not going away. An understanding of what linked data has to offer is akin to experiencing Rome first hand. Both will ultimately broaden your perspective. Consequently it is a good idea to make a concerted effort to learn about linked data, as well as visit Rome at least once. Once you have returned from your trip, discuss what you learned with your friends, neighbors, and colleagues. The result will be enlightening everybody.

The previous sections of this book described what linked data is and why it is important. The balance of book describes more of the how's of linked data. For example, there is a glossary to help reenforce your knowledge of the jargon. You can learn about HTTP "content negotiation" to understand how actionable URIs can return HTML or RDF depending on the way you instruct remote HTTP servers. RDF stands for "Resource Description Framework", and the "resources" are represented by URIs. A later section of the book describes ways to design the URIs of your resources. Learn how you can transform existing metadata records like MARC or EAD into RDF/XML, and then learn how to put the RDF/XML on the Web. Learn how to exploit your existing databases (such as the one's under Archon, Archivist's Toolkit, or ArchiveSpace) to generate RDF. If you are the Do It Yourself type, then play with and explore the guidebook's tool section. Get the gentlest of introductions to searching RDF using a query language called SPARQL. Learn how to read and evaluate ontologies & vocabularies. They are manifested as XML files, and they are easily readable and visualizable using a number of programs. Read about and explore applications using RDF as the underlying data model. There are a growing number of them. The book includes a complete publishing system written in Perl, and if you approach the code of the publishing system as if it were a theatrical play, then the "scripts" read liked scenes. (Think of the scripts as if they were a type of poetry, and they will come to life. Most of the "scenes" are less than a page long. The poetry even includes a number of refrains. Think of the publishing system as if it were a one act play.) If you want to read more, and you desire a vetted list of books and articles, then a later section lists a set of further reading.

After you have spent some time learning a bit more about linked data, discuss what you have learned with your colleagues. There are many different aspects of linked data publishing, such as but not limited to:

  * allocating time and money 
  * analyzing the RDF of yours as well as others
  * articulating policies
  * cleaning and improving RDF 
  * collecting and harvesting the RDF of others
  * deciding what ontologies & vocabularies to use
  * designing local URIs
  * enhancing RDF triples stores by asserting additional relationships
  * finding and identifying URIs for the purposes of linking
  * making RDF available on the Web (SPARQL, RDFa, data dumps, etc.)
  * project management
  * provisioning value-added services against RDF (catalogs, finding aids, etc.)
  * storing RDF in triple stores

In archival practice, each of these things would be done by different sets of people: archivists & content specialists, administrators & managers, computer programers & systems administrators, metadata experts & catalogers. Each of these sets of people have a piece of the publishing puzzle and something significant to contribute to the work. Each of these sets of people plays a key and indispensable role in linked data publishing:

  * archivists & content specialists - These are the people who understand the "aboutness" of a particular collection. These are the people who understand and can thoroughly articulate the significance of a collection. They know how and why particular things belong in a collection. They are able to answer questions about the collection as all as tell stories against it. 

  * administrators & managers - These are "resource allocators". They are people who manage time and money. They are people who establish priorities on an institutional level. They are expected to be financially aware and politically savvy. These people will have a view of the wider environment, have their finger on the pulse of where the local institution is moving, and be able to juggle seemingly conflicting directives. More than anybody else, they are expected to be able to outline a plan and see it to fruition.
  
  * metadata specialists & catalogers - These are people who understand data about data. Not only do they understand the principles of controlled vocabularies and authority lists, but they are also familiar with a wide variety of such lists, specifically as they are represented on the Web. In linked data there are fewer descriptive cataloging "rules". Nevertheless, the way the ontologies of linked data can be used need to be interpreted, and this interpretation needs to be consistent. Metadata specialists understand these principles.

  * computer programers & systems administrators - Not only are these the people who have a fundamental understanding of what computer can and cannot do, but they also know how to put this understanding into practice. At the very least, the computer technologists need to understand a myriad of data structures and how to convert them into different data structures. Converting MARC 21 into MARCXML. Transforming EAD into HTML. Reporting against a relational database to create serialized RDF. These tasks required computer programming skills, but not necessarily any one in particular. Any modern programming language (Java, PHP, Python, Ruby, etc.) includes the necessary function to complete the tasks. 

Read about linked data. Learn about linked data. Bring these sets of people together discuss what you have learned. With this in mind, articulate some goals — broad targets of things you would like to accomplish. Some of them might include:

  * making your archival collections more widely accessible
  * working with others to build virtual collections of like topics or formats
  * incorporating your archival descriptions into public spaces like Wikipedia
  * integrating your collections into local teaching, learning, and research activities
  * increasing the awareness of your archive to benefactors
  * increasing the computer technology skills of fellow archivists

The what of your objectives are not so much identified with nouns as they are action verbs, such as: write, evaluate, implement, examine, purchase, hire, prioritize, list, delete, acquire, discuss, share, find, compare & contrast, stop, start, complete, continue, describe, edit, updated, create, purchase, upgrade, etc. The what of your objective is in the doing.

After discussing these sorts of issues, at the very least you will have a better collective understanding of the possibilities. If you don't plan to "go to Rome" right away, you might decide to reconsider the "vacation" at another time.

  "Even Michelangelo, when he painted the Sisten Chapel, worked
  with a team of people each possessing a complementary set of
  skills. Each had something different to offer, and the
  discussion between themselves was key to their success."


Rome in three days

  "If you to go to Rome for a few days, do everything you would do
  in a single day, eat and drink in a few cafes, see a few
  fountains, and go to a museum of your choice."

Linked data in archival practice is not new. Others have been here previously. You can benefit from their experience and begin publishing linked data right now using tools with which you are probably already familiar. For example, you probably have EAD files, sets of MARC records, or metadata saved in database applications. Using existing tools, you can transform this content into RDF and put the result on the Web, thus publishing your information as linked data.

At its very root, linked data is about making your data available for others to harvest and use. While the "killer linked data application" has seemingly not reared its head, this does not mean you ought not make your data available at linked data. You won't see the benefits immediately, but sooner or later (less than 5 years from now), you will see your content creeping into the search results of Internet indexes, into the work of both computational humanists and scientists, and into the hands of esoteric hackers creating one-off applications. Internet search engines will create "knowledge graphs", and they will include links to your content. The humanists and scientists will operate on your data similarly. Both will create visualizations illustrating trends. They will both quantifiably analyze your content looking for patterns and anomalies. Both will probably create network diagrams demonstrating the flow and interconnection of knowledge and ideas through time and space. The humanist might do all this in order to bring history to life or demonstrate how one writer influenced another. The scientist might study ways to efficiently store your data, easily move it around the Internet, or connect it with data set created by their apparatus. The hacker (those are the good guys) will create flashy-looking applications that many will think are weird and useless, but the applications will demonstrate how the technology can be exploited. These applications will inspire others, be here one day and gone the next, and over time, become more useful and sophisticated. 


EAD

If you have used EAD to describe your collections, then you can easily make your descriptions available as valid linked data, but the result will be less than optimal. This is true not for a lack of technology but rather from the inherent purpose and structure of EAD files.

A few years ago an organisation in the United Kingdom called the Archive's Hub was funded by a granting agency called JISC to explore the publishing of archival descriptions as linked data. The project was called LOCAH. [1] One of the outcomes of this effort was the creation of an XSL stylesheet (ead2rdf) transforming EAD into RDF/XML. [2] The terms used in the stylesheet originate from quite a number of standardized, widely accepted ontologies, and with only the tiniest bit configuration / customization the stylesheet can transform a generic EAD file into valid RDF/XML for use by anybody. The resulting XML files can then be made available on a Web server or incorporated into a triple store. This goes a long way to publishing archival descriptions as linked data. The only additional things needed are a transformation of EAD into HTML and the configuration of a Web server to do content negotiation between the XML and HTML. 

For the smaller archive with only a few hundred EAD files whose content does not change very quickly, this is a simple, feasible, and practical solution to publishing archival descriptions as linked data. With the exception of doing some content negotiation, this solution does not require any computer technology that is not already being used in archives, and it only requires a few small tweaks to a given workflow:

  1. implement a content negotiation solution 
  2. create and maintain EAD file s
  3. transform EAD into RDF/XML 
  4. transform EAD into HTML 
  5. save the resulting XML and HTML files on a Web server 
  6. go to step #2

EAD is a combination of narrative description and a hierarchal inventory list, and this data structure does not lend itself very well to the triples of linked data. For example, EAD headers are full of controlled vocabularies terms but there is no way to link these terms with specific inventory items. This is because the vocabulary terms are expected to describe the collection as a whole, not individual things. This problem could be overcome if each individual component of the EAD were associated with controlled vocabulary terms, but this would significantly increase the amount of work needed to create the EAD files in the first place.

The common practice of using literals to denote the names of people, places, and things in EAD files would also need to be changed in order to fully realize the vision of linked data. Specifically, it would be necessary for archivists to supplement their EAD files with commonly used URIs denoting subject headings and named authorities. These URIs could be inserted into id attributes throughout an EAD file, and the resulting RDF would be more linkable, but the labor to do so would increase, especially since many of the named items will not exist in standardized authority lists.

Despite these short comings, transforming EAD files into some sort of serialized RDF goes a long way towards publishing archival descriptions as linked data. This particular process is a good beginning and outputs valid information, just information that is not as linkable as possible. This process lends itself to iterative improvements, and outputting something is better than outputting nothing. But this particular proces is not for everybody. The archive whose content changes quickly, the archive with copious numbers of collections, or the archive wishing to publish the most complete linked data possible will probably not want to use EAD files as the root of their publishing system. Instead some sort of database application is probably the best solution.


MARC

In some ways MARC lends it self very well to being published via linked data, but in the long run it is not really a feasible data structure.

Converting MARC into serialized RDF through XSLT is at least a two step process. The first step is to convert MARC into MARCXML and then MARCXML into MODS. This can be done with any number of scripting languages and toolboxes. The second step is to use a stylesheet such as the one created by Stefano Mazzocchi to transform the MODS into RDF/XML -- mods2rdf [3] From there a person could save the resulting XML files on a Web server, enhance access via content negotiation, and called it linked data.

Unfortunately, this particular approach has a number of drawbacks. First and foremost, the MARC format had no place to denote URIs; MARC records are made up almost entirely of literals. Sure, URIs can be constructed from various control numbers, but things like authors, titles, subject headings, and added entries will most certainly be literals ("Mark Twain", "Adventures of Huckleberry Finn", "Bildungsroman", or "Samuel Clemans"), not URIs. This issue can be overcome if the MARCXML were first converted into MODS and URIs were inserted into id or xlink attributes of bibliographic elements, but this is extra work. If an archive were to take this approach, then it would also behoove them to use MODS as their data structure of choice, not MARC. Continually converting from MARC to MARCXML to MODS would be expensive in terms of time. Moreover, with each new conversion the URIs from previous iterations would need to be re-created.


EAC-CPF

Encoded Archival Context for Corporate Bodies, Persons, and Families (EAC-CPF) goes a long way to implementing a named authority database that could be linked from archival descriptions. [4] These XML files could easily be transformed into serialized RDF and therefore linked data. The resulting URIs could then be incorporated into archival descriptions making the descriptions richer and more complete. For example the FindAndConnect site in Australia uses EAC-CPF under the hood to disseminate information about people in its collection. [5] Similarly, "SNAC aims to not only make the [EAC-CPF] records more easily discovered and accessed but also, and at the same time, build an unprecedented resource that provides access to the socio-historical contexts (which includes people, families, and corporate bodies) in which the records were created" [6] More than a thousand EAC-CPF records are available from the RAMP project. [7]


METS, MODS, OAI-PMH service providers, and perhaps more

If you have archival descriptions in either of the METS or MODS formats, then transforming them into RDF is as far away as your XSLT processor and a content negotiation implementation. As of this writing there do not seem to be any METS to RDF stylesheets, but there are a couple stylesheets for MODS. The biggest issue with these sorts of implementations are the URIs. It will be necessary for archivists to include URIs into as many MODS id or xlink attributes as possible. The same thing holds true for METS files except the id attribute is not designed to hold pointers to external sites.

Some archives and libraries use a content management system called ContentDM. [8] Whether they know it or not, ContentDM comes complete with an OAI-PMH (Open Archives Initiative - Protocol for Metadata Harvesting) interface. This means you can send a REST-ful URL to ContentDM, and you will get back an XML stream of metadata describing digital objects. Some of the digital objects in ContentDM (or any other OAI-PMH service provider) may be something worth exposing as linked data, and this can easily be done with a system called oai2lod. [9] It is a particular implementation of D2RQ, described below, and works quite well. Download application. Feed oai2lod the "home page" of the OAI-PMH service provider, and oai2load will publish the OAI-PMH metadata as linked open data. This is another quick & dirty way to get started with linked data.


Databases

Publishing linked data through XML transformation is functional but not optimal. Publishing linked data from a database comes closer to the ideal but requires a greater amount of technical computer infrastructure and expertise. 

Databases -- specifically, relational databases -- are the current best practice for organizing data. As you may or may not know, relational databases are made up of many tables of data joined together with keys. For example, a book may be assigned a unique identifier. The book has many characteristics such as a title, number of pages, size, descriptive note, etc. Some of the characteristics are shared by other books, like authors and subjects. In a relational database these shared characteristics would be saved in additional tables, and they would be joined to a specific book through the use of unique identifiers (keys). Given this sort of data structure, reports can be created from the database describing its content. Similarly, queries can be applied against the database to uncover relationships that may not be apparent at first glance or buried in reports. The power of relational databases lies in the use of keys to make relationships between rows in one table and rows in other tables. The downside of relational databases as a data model is infinite variety of fields/table combinations making them difficult to share across the Web.

Not coincidently, relational database technology is very much the way linked data is expected to be implemented. In the linked data world, the subjects of triples are URIs (think database keys). Each URI is associated with one or more predicates (think the characteristics in the book example). Each triple then has an object, and these objects take the form of literals or other URIs. In the book example, the object could be "Adventures Of Huckleberry Finn" or a URI pointing to Mark Twain. The reports of relational databases are analogous to RDF serializations, and SQL (the relational database query language) is analogous to SPARQL, the query language of RDF triple stores. Because of the close similarity between well-designed relational databases and linked data principles, the publishing of linked data directly from relational databases makes whole lot of sense, but the process requires the combined time and skills of a number of different people: content specialists, database designers, and computer programmers. Consequently, the process of publishing linked data from relational databases may be optimal, but it is more expensive.

Thankfully, many archivists probably use some sort of behind the scenes database to manage their collections and create their finding aids. Moreover, archivists probably use one of three or four tools for this purpose: Archivist's Toolkit, Archon, ArchivesSpace, or PastPerfect. Each of these systems have a relational database at their heart. Reports could be written against the underlying databases to generate serialized RDF and thus begin the process of publishing linked data. Doing this from scratch would be difficult, as well as inefficient because many people would be starting out with the same database structure but creating a multitude of varying outputs. Consequently, there are two alternatives. The first is to use a generic database application to RDF publishing platform called D2RQ. The second is for the community to join together and create a holistic RDF publishing system based on the database(s) used in archives.

D2RQ is a very powerful software system. [10] It is supported, well-documented, executable on just about any computing platform, open source, focused, functional, and at the same time does not try to be all things to all people. Using D2RQ it is more than possible to quickly and easily publish a well-designed relational database as RDF. The process is relatively simple:

  * download the software 

  * use a command-line utility to map the database
    structure to a configuration file 

  * edit the configuration file to meet your needs 

  * run the D2RQ server using the configuration file
    as input thus allowing people or RDF user-agents
    to search and browse the database using linked
    data principles 

  * alternatively, dump the contents of the database
    to an RDF serialization and ingest the result
    into your favorite RDF triple store

The downside of D2RQ is its generic nature. It will create an RDF ontology whose terms correspond to the names of database fields. These field names do not map to widely accepted ontologies & vocabularies and therefore will not interact well with communities outside the ones using a specific database structure. Still, the use of D2RQ is quick, easy, and accurate.


Triple stores and SPARQL endpoints

Publishing your RDF as static files is relatively easy, but in order to really take advantage of your efforts you will want to additionally save your RDF in a triple store. There are many open source and commercial applications from which to choose. They all come with a set of specialized features, but they all work essentially the same way. Install. Configure. Run application to ingest RDF. Use different functions to edit and report on the contents of the triple store. Comparing & contrasting each of the available triple stores is beyond the scope of this document, but at the very least, look for one that provides access via a SPARQL endpoint. SPARQL is the query language of RDF triple stores, but the query language goes beyond the generation of lists. It can also be used to assert new RDF statement and have them saved to the store. It provides the means for answering Boolean questions or performing mathematical functions. Consequently, SPARQL can be used to not only search for information but also create new information and answer real-world collections.

Simply publishing RDF as static files is only half of the problem to be solved. By saving your RDF in a triple store and providing access to it via a SPARQL endpoint you open yourself up to possibilities that go beyond the dissemination of finding aids and description. Creating triple stores of your RDF (as well as the RDF of others) enable to tell more compelling stories and succinctly answer given questions. 


Talk with your colleagues

Do not keep your candle hidden beneath a bushel basket. Tell your colleagues about your accomplishments. Write about your experience and share it via mailing lists, blog posting, conference presentations, formally published articles, and book. Through the writing process you will become more aware of both your successes as well as your failures. Others will see what you have done and offer comments. Some will follow your lead. Others will take your lead and go further. In any case, the dialog about linked data -- in terms of both its strengths & weaknesses -- will grow louder. Such dialog can only be a good thing. More useful techniques will be come best practices, and less useful techniques will fade from existence. Dialoge builds community, and communities can become very strong. A particular proverb comes to mind. "If you want to go fast, then go alone. If you want to go far, take your friends." Work in the sphere of linked data is exactly like this. Manifesting your archival description by transforming EAD into RDF is all well and good. It is fast & easy, but if you want to truly take advantage of the "linking" in linked data, then you will need to talk with your colleagues -- both in and out of archives -- in order to take things to another level.

  "If you are going to be in Rome for only a few days, you will
  want to see the major sites, and you will want to adventure out &
  about a bit, but at the same time is will be a wise idea to
  follow the lead of somebody who has been there previously. Take
  the advise of these people. It is an efficient way to see some of
  the sights."


Rome in week

  "For a week, do everything you would do in a few days, and make
  one or two day-trips outside Rome in order to get a flavor of the
  wider community."

In this "itinerary" you begin publishing RDF, but at the same time you actively design and document  URIs as well as select ontologies & vocabularies for your RDF. You then re-publish your existing data using your documentation as a guide. And you re-implement your SPARQL endpoint. Discuss linked data not only with your colleagues but with people outside archival practice. This particular strategy is probably the most difficult it requires a lot of re-thinking about archival practice and ways it can be manifested. To these ends, there are three books of particular interest to the reader. Think of them as required reading:

  1. Linked data: Evolving the Web into a global data space by Tom Heath and Christian Bizer [11] - This book provides a thorough and up-to-date overview of what linked data is and why it is important. It is full of useful examples and recipes for implementation.
  
  2. Semantic Web for the working ontologist: Effective modeling in RDFS and OWL buy Dean Allemang and James Hendler [12] - This book is primarily intended for people who are evaluating and designing ontologies & vocabularies. It compares & contrasts RDF to alternative data models (like spreadsheets or relational databases), and explains why RDF is particularly suitable to the Web. It then outlines how to create one's own ontology.
  
  3. Linked data patterns: A Pattern catalogue for modeling, publishing, and consuming linked data by Leigh Dodds and Ian Davis [13] - As a "pattern catalog", this book is a long list of common problem experienced by people implementing linked data as well as accompanying discussion and solutions to the problems. The problems and solutions are divided into a number of categories: URIs (identifiers), modeling, publishing, data management, and applications. 


Design your URIs

The heart of linked data is the description of things. Things can be anything, either real or imaginary, and all of these things are expected to be represented by URIs. Consequently, when you are intending to publish linked data, you need to think about the structure of the identifiers of your things. To large degree URIs are expected to be "cool". [14] This means they should not change and they should be:

  * simple - short, human-readable without any name/value pairs,
    and easily depicted in written form

  * stable - immutable, something that lasts two, twenty, or two
	hundred years, sans implementation bits such as file name
	extensions because file types (JPEG, PDF, etc.) change over time

  * manageable - meaning you have some sort of system for dealing
	with implement changes behind the scenes; content negotiation
	systems and HTTP server management systems are keys to success
  
Pete Johnson, while designing the URIs for the "things" in LOCAH, articulated the following pattern for the URIs he was going to mint [15]:

  http://{domain}/id/{concept}/{reference}

Examples then included:

  * http://example.org/id/findingaid/mums059

  * http://example.org/id/family/clarkfamily

  * http://example.org/id/repository/usUS-DLC

These URIs were derived from the data inside EAD files and took into account:

  * local identifiers - URIs based embedded identifiers

  * authority controlled values - building from the names in
    remote lists

  * locally scoped names - building from the names in local lists

  * locally developed "rules" - identifiers based on long strings
    of text

  * identifier inheritance - URIs starting with local identifiers
    and building on hierarchy

These principles are not very much different from some of the Identifier Patterns articulated by Dodds and Davis. Patterns included:

  * hierarchical URIs - these are "patterned" URIs denoting sub and
    super-class relationships

  * literal keys - based on custom properties, such as a sub-class
    if Dublin Core identifiers

  * natural keys - URIs built from things like ISSN numbers, OCLC
    numbers, database keys, etc.

  * patterned URIs - designed after predicable schemes, much like
    Johnson's examples

  * proxy URIs - URIs created after some alignment is done against
    amalgamated RDF

  * rebased URIs - a renamed URI done so because the previous one
    was not as "cool" as it could be

  * shared keys - identifiers created across domains

  * URL slugs - patterned URIs based based on strings

Designing your URIs is a bit of a chicken & egg problem. You are designing URIs for the "things" of your archival description, and before you can create the URIs you need to know what "things" you have, which is the subject of the next section. On the other hand, the subjects and objects of RDF statements are expected to be URIs -- the identifiers you will be sharing -- the very heart of linked data. In other words, you need both URIs as well as ontologies & vocabularies at once and at the same time to do the work of linked data. They go hand-in-hand. You can not do one without the other. 

At the very least remember one thing, design your URIs to be "cool". 


Select your ontology & vocabularies

Articulate and implement best practices for publishing RDF - Work with your friends to articulate and document an "application profile". As guidelines and best practices get articulated, implement them by going back to Step #1. In the meantime, continue on to Step #4.
  

Republish your RDF

The second alternative requires community effort and coordination. The databases of Archivist's Toolkit, Archon, ArchivesSpace, or Past Perfect could be assumed. The community could then get together and decide on an RDF ontology to use for archival descriptions. The database structure(s) could then be mapped to this ontology. Next, programs could be written against the database(s) to create serialized RDF thus beginning the process of publishing linked data. Once that was complete, the archival community would need to come together again to ensure it uses as many shared URIs as possible thus creating the most functional sets of linked data. This second alternative requires a significant amount of community involvement and wide-spread education. It represents a never-ending process.

Publish your RDF -2. Publish RDF - No matter what kind of RDF you are able to create, make it available on the Web 1. Do simple publishing - At its very root, linked data is about making your data available for others to harvest and use. While the "killer linked data application" has seemingly not reared its head, this does not mean you ought not make your data available at linked data. You won't see the benefits immediately, but sooner or later (less than 5 years from now), you will see your content creeping into the search results of Internet indexes, into the work of both computational humanists and scientists, and into the hands of esoteric hackers creating one-off applications. Internet search engines will create "knowledge graphs", and they will include links to your content. The humanists and scientists will operate on your data similarly. Both will create visualizations illustrating trends. They will both quantifiably analyze your content looking for patterns and anomalies. Both will probably create network diagrams demonstrating the flow and interconnection of knowledge and ideas through time and space. The humanist might do all this in order to bring history to life or demonstrate how one writer influenced another. The scientist might study ways to efficiently store your data, easily move it around the Internet, or connect it with data set created by their apparatus. The hacker (those are the good guys) will create flashy-looking applications that many will think are weird and useless, but the applications will demonstrate how the technology can be exploited. These applications will inspire others, be here one day and gone the next, and over time, become more useful and sophisticated. 


Discuss linked data with people outside archival practice


===========================


Rome in two weeks

  "If you can afford two weeks, then do everything you would do in
  a week, and in addition befriend somebody in the hopes of
  establishing a life-long relationship."

Now that you have significantly practiced with the principles of publishing linked data, it is time to harvest the RDF of others so you can enhance your archival services. To do this you will first continue to do everything in the previous sections. You will then supplement and enhance your triple store with the RDF and information of others'. Third, you will write applications against the triple store that go beyond search. These services will be of two types: 1) services for curating the collection of information, and 2) services for using the collection. The former services are primarily for archivists and metadata specialists. The later are primarily intended for everybody from the general public to the scholar. These services will be akin to the telling of stories, and once implemented you will be discussing linked data with the world, literally.


Supplement and enhance your triple store

RDF is published in many different ways such as but not limited to: as triple store dumps, via content negotiation on the other side of actionable URIs, embedded in HTML files as RDFa, available as the result of SPARQL queries, etc. Other metadata and information is available from other means such at REST-ful websites, OAI-PMH data providers, spreadsheets and databases from communities of interest. The information and metadata from each of these "places" can be identified and prioritized into some sort of collection development policy. For example, you might be interested in augmenting your RDF with the RDF of other archival collections. You might want to supplement your descriptions with images of people, places, and things. The places in your archival description may be enhanced with geographic coordinates. Controlled vocabularies in your description may be equivocated or related to other  controlled vocabularies enabling the interlinking across collections. What ever the reason, the harvesting and collecting of other people's metadata and information can only enrich the information services you provide.

Begin the harvesting process by taking advantages of directories of RDF and metadata sets. Datahub is a good example. [16]  Dumps of RDF could be mirrored locally and then ingested into the triple store. HTML pages could be crawled, RDFa extracted, and the result ingested into the triple store. Lists of actionable URIs could be created by searching remote websites. These actionable URIs could then be fed to a computer program which will use content negotiation to harvest the remote RDF. The actual harvesting process will almost definitely require the skills of a computer programmer or systems administrator. After remote metadata and RDF has been incorporated into your local triple store, you will want to continue to implement your local SPARQL endpoint, because you will definitely use SPARQL to implement the triple store curation services as well as the services for the general public and scholar. 

 
Curating the collection

As additional information is brought into your triple store, there will be an increasing need to curate the information. This process is very similar, if not congruent with the processes behind a project of a few years ago called the National Science Foundation Digital Library (NSDL). [17] This project was intended to harvest data from across the Internet, amalgamate it, and provide services against the result. These services often included creating subsets of the data and search services on top of them. This curation process is similar to the collection / services processes undertaken by Europe's Europeana project and the Digital Public Library of America.

Once the metadata and information have been acquired there is a need to make it the "best" information possible. What does this mean? In this case, the word "best" connotes information that is: 1) consistent, 2) correct, 3) accurate, 4) complete, and 5) timely. The processes of maintaining metadata and information are just as ongoing and indeterminate as the curation of physical collection.

Creating and maintaining metadata is a never-ending process. The items being described can always use elaboration. Collections may increase is size. Rights applied against content may change. Things become digitized, or digitized things are migrated from one format to another. Because of these sorts of things and many others, cleanup, conversion, and consistency are something every metadata specialist needs to keep in mind. They are things to be managed and maintained as the linked data of others is assimilated into your own collection. These issues have been nicely elaborated upon in separate articles by Dianne Hillman and Thomas Johnson. [18, 19]. Hillman outlines a number of problems:

  1. missing data - metadata elements not present in supplied
     metadata

  2. incorrect data - metadata values not conforming to standard
     element use

  3. confusing data - multiple values crammed into a single
     metadata element, embedded HTML tags, etc.

  4. insufficient data - no indication of controlled vocabularies
     used 

Johnson similar things:

  1. removing "noise" - removing named labels with no values (empty
	 elements), removing statements with non-information whose values
	 are things like "unknown" or "n/a", or values whose content is
	 only punctuation

  2. normalizing presentation - removing extraneous white spaces,
	 removing HTML double encodings, normalizing the order of first
	 names and last names, etc.

  3. assigning URIs to curation objects - identifying URIs for
	 string values, assigning URIs to digitized objects

  4. mapping legacy elements to linked data vocabularies - for
	 example, transforming any elements with values like "245" to some
	 form of title

Linked data affords an additional type of enhancement -- enhancements of relationship and augmentation. RDF is made up of ontologies. Statements will be asserted and brought together in similar collection. One set of statements may use FOAF. Another may use Dublin Core. These two ontologies include the concept of names of people. You might want to make assertions in your RDF equivocating the names in FOAF with the names in Dublin Core. Similar things may be done with identifiers where URIs of this type are intended to be equivalent with the URIs of another type. For example, the URI denoting Mark Twain in VIAF may be equivalent to the URI RDF collection. By denoting equivalence between these two items additional information can be brought to bear regarding Mark Twain.

Cleanup, conversion, and consistency mean many things. Does all of your metadata use the same set of one or more vocabularies? Are things spelled correctly? Maybe you used abbreviations in one document but spelled things out in another? Have you migrated your JPEG images to JPEG2000 or TIFF formats? Maybe the EAD DTD has been updated, and you want (need) to migrate your finding aids from one XML format to another? Do all of your finding aids exhibit the same level of detail; are some "thinner" than others? Have you used one form of a person's name in one document but used another form in a different document? The answers to these sorts of questions point to the need for cleanup, conversion, and consistency. 

Is your archival description LOD-ready? Now? The simple, straight-forward answer is, "Yes." The longer and more complicated answer is, "No. Your data is never 100% linked data ready because the process of archival description is never finished."


Applications & use-cases

In the previous "tours of Rome", the creation of a simple search engine against your triple store was suggested. In those "itineraries" other applications could have been created, but now that additional metadata and information have been brought to light it will be behoove you to go beyond the simple search engine and begin to tell stories. If not, then the services you are providing aren't very much dissimilar to a search against Google. Here are a few application and use-case ideas.

Create a union catalog. This is really an enhancement of the simple search idea. If you make your data available as linked data, and if you find at least one other archive who is making their data available as linked data, then you can find a third somebody who will combine them into a triple store and implement a rudimentary SPARQL interface against the union. Once this is done a researcher could conceivably search the interface for a URI to see what is in both collections. The absolute imperative key to success for this to work is the judicious inclusion of URIs in both data sets. This scenario becomes even more enticing with the inclusion of two additional things. First, the more collections in the triple store the better. You can not have enough collections in the store. Second, the scenario will be even more enticing when each archive publishes their data using similar ontologies as everybody else. Success does not hinge on similar ontologies, but success is significantly enhanced. Just like the relational databases of today, nobody will be expected to query them using their native query language (SQL or SPARQL). Instead the interfaces will be much more user-friendly. The properties of classes in ontologies will become facets for searching and browsing. Free text as well as fielded searching via drop-down menus will become available. As time goes on and things mature, the output from these interfaces will be increasingly informative, easy-to-read, and computable. This means the output will answer questions, be visually appealing, as well as be available in one or more formats for other computer programs to operate upon.  

Tell a story. You and your hosting institution(s) have something significant to offer. It is not just about you and your archive but also about libraries, museums, the local municipality, etc. As a whole you are a local geographic entity. You represent something significant with a story to tell. Combine your linked data with the linked data of others in your immediate area. The ontologies will be a total hodgepodge, at least at first. Now provide a search engine against the result. Maybe you begin with local libraries or museums. If you work in an academic setting, then maybe you begin with other academic departments across campus. Allow people to search the interface and bring together the content of everybody involved. Do not just provide lists of links in search results, but instead create knowledge graphs. Supplement the output of search results with the linked data from Wikipedia, Flickr, etc. In a federated search sort of way, supplement the output with content from other data feeds such as (licensed) bibliographic indexes or content harvested from OAI-PMH repositories. Identify complementary content from further afield. Figure out a way for you and they to work together to create a newer, more complete set of content. Creating these sorts of things on-the-fly will be challenging. On the other hand, you might implement something that is more iterative and less immediate, but more thorough and curated if you were to select a topic or theme of interest, and do your own searching and story telling. The result would be something that is at once a Web page, a document designed for printing, or something importable into another computer program. 

Create new knowledge. Create an inference engine, turn it against your triple store, and look for relationships between distinct sets of URIs that weren't previously apparent. Here's one way how: 

  1. allow the reader to select an actionable URI of personal
     interest, ideally a URI from the set of URIs you curate

  2. submit it an HTTP server or SPARQL endpoint and request RDF as
     output

  3. save the output to a local store

  4. for each subject and object URI found the output, go to
     Step #2

  5. go to step #2 n times for each newly harvested URI in the store
     where n is a reader-defined integer greater than 1; in other
     words, harvest more and more URIs, predicates, and literals
     based on the previously harvested URIs

  6. create a set of human readable services / reports against the
     content of the store, and think of these services / reports akin to
     finding aids, reference materials, or museum exhibits of the
     future: Example services / reports might include:

      * hierarchal lists of all classes and properties - This
        would be a sort of semantic map. Each item on the map
        would be clickable allowing the reader to read more and
        drill down.

      * text mining reports - collect into a single "bag of
        words" all the literals saved in the store and create:
        word clouds, alphabetical lists, concordances,
        bibliographies, directories, gazetteers, tabulations of
        parts of speech, named entities, sentiment analyses,
        topic models, etc.

      * maps - use place names and geographic coordinates to
        implement a geographic information service

      * audio-visual mash-ups - bring together all the media
        information and create things like slideshows, movies,
        analyses of colors, shapes, patterns, etc.

      * search interfaces - implement a search interface
        against the result, SPARQL or otherwise

      * facts - remember SPARQL queries can return more than
        just lists. They can return mathematical results such
        as sums, ratios, standard deviations, etc. It can also
        return Boolean values helpful in answering yes/no
        questions. You could have a set of canned fact queries
        such as, how many ontologies are represented in the
        store. Is the number of ontologies greater than 3? Are
        there more than 100 names represented in this set? The
        count of languages used in the set, etc.

  7. Allow the reader to identify a new URI of personal interest,
     specifically one garnered from the reports generated in Step #5.

  8. Go to Step #2, but this time have the inference engine be more
     selective by having it try to crawl back to your namespace and
     set of locally curated URIs.

  9. Return to the reader the URIs identified in Step #7, and by
     consequence, these URIs ought to share some of the same
     characteristics as the very first URI; you have implemented a
     "find more like this one" tool. You, as curator of the collection
     of URIs might have thought the relations between the first URI
     and set of final URIs was obvious, but those relationships would
     not necessarily be obvious to the reader, and therefore new
     knowledge would have been created or brought to light.

 10. If there are no new URIs from Step #7, then go to Step #6
     using the newly harvested content.

 11. Done - if a system were created such as the one above, then
     the reader would quite likely have acquired some new knowledge,
     and this would be especially true the greater the size of n in
     Step #5. 

Expand beyond the document centric finding aid model. The archival finding aid (specifically the EAD file) is essentially a document with two parts: 1) a narrative story describing a collection, and 2) an inventory of items in the collection. Yes, these finding aids are manifested as XML files, and therefore they are well-structured and computer-readable, but they do not really take advantage of the "webbed" environment. Finding aids impose a conceptual model on a collection from the point of view of the archivist. This in an of itself is not a bad thing and it serves many purposes. At the same time, finding aids make it difficult to assert alternative conceptual models on the same collection. Like the MARC records of librarianship, EAD files are an electronic form of a print data model. MARC is to catalog cards as EAD is to finding aids. Each serves a particular purpose, but neither exploit nor really take advantage of computers connected through HTTP. Migrating archival description from document-centric finding aids to the more atomistic RDF makes it easier for many models to be asserted. It also overcomes a number of other limitations. The following use-cases are gleaned from the original LiAM proposal and can be addressed by manifesting archival description as linked data:

  * An individual record or series of records often have
	simultaneous significance in multiple contexts. How do you
	signify simultaneous records when they might have multiple
	contexts? --Anne Sauer

  * Record creators have multifaceted relationships to different
	records and series of records. How do you model record creators
	with different series of records? --Anne Sauer

  * Documentation of a function often spans provenance-based
	records series. How do you link functions when provenance-based
	records span series? --Anne Sauer

  * Documentation of an event often spans provenance-based records
	series. How do you link events when provenance-based records span
	series? --Anne Sauer

  * Intentional filing systems are often no longer employed as a
	file management strategy, undermining the affordance of original
	order as a conceptual basis for description. How do you model the
	intentional filing system? --Anne Sauer

  * Because women may change their names more frequently than men
	in Western culture, how do you create connections between
	collections due to different names for the same person. An
	interesting example of scattered collections that would have been
	brought together are the William Cameron Blackett personal
	archive at the Harvard University Archives with two collections
	of papers of his daughter (Priscilla Blackett Dewey Houghton)
	described as the Priscilla B. Dewey papers at the Harvard Theatre
	Collection, Houghton Library and the papers of Priscilla Dewey
	Houghton at the Schlesinger Library. --Juliana Kuipers

  * How do you account for migrating responsibility? Responsibility
	and authority for a particular administrative function may move
	from one office to another. For example, student discipline may
	move from a President's office to a Dean of a school, to the Dean
	of Students, and then to a disciplinary board. --Kate Bowers

  * How do you account for multiple email addresses? Multiple
	aliases for the same email account may exist simultaneously, one
	person may have many email accounts. --Skip Kendall

  "Going to Rome for two weeks means going beyond the usual tourist
  sights and really delving into the culture. By doing so you will
  learn of the wider world around you, break out any silos, and
  become a more active participant in a global conversation. Not
  only that, but you will be doing it in a more universal and
  timeless fashion."


Links

[1] LOCAH - http://archiveshub.ac.uk/locah/

[2] ead2rdf - http://data.archiveshub.ac.uk/xslt/ead2rdf.xsl

[3] mods2rdf - https://github.com/dltj/MARC-MODS-RDFizer/blob/master/stylesheets/mods2rdf.xslt

[4] EAC-CPF - http://eac.staatsbibliothek-berlin.de/

[5] FindAndConnect - http://www.findandconnect.gov.au

[6] SNAC - http://socialarchive.iath.virginia.edu

[7] RAMP - http://demo.rampeditor.info/export.php

[8] ContentDM - http://www.contentdm.org

[9] oai2lod - https://github.com/behas/oai2lod

[10] D2RQ - http://d2rq.org

[11] Linked data: Evolving the Web into a global data space by Tom Heath and Christian Bizer - http://linkeddatabook.com

[12] Semantic Web for the working ontologist: Effective modeling in RDFS and OWL buy Dean Allemang and James Hendler - http://workingontologist.org

[13] Linked data patterns: A Pattern catalogue for modeling, publishing, and consuming linked data by Leigh Dodds and Ian Davis - http://patterns.dataincubator.org

[14] "cool" URIs - http://www.w3.org/TR/cooluris/

[15] Identifying the “things”: URI Patterns for the Hub Linked Data by Pete Johnson - http://archiveshub.ac.uk/locah/2010/11/identifying-the-things-uri-patterns-for-the-hub-linked-data/

[16] Datahub - http://datahub.io/

[17] NSDL - https://nsdl.org

[18] Improving Metadata Quality: Augmentation and Recombination by Diane Hillmann, Naomi Dushay, and Jon Phipps - http://dcpapers.dublincore.org/pubs/article/view/770/766

[19] Recipes for Enhancing Digital Collections with Linked Data by Thomas Johnson and Karen Estlund - http://journal.code4lib.org/articles/9214

